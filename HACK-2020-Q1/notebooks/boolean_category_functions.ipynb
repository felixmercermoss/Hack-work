{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign boolean category labels to news articles\n",
    "\n",
    "### Datascapes Hack, June 2020\n",
    "Hack documentation can be found [here](https://paper.dropbox.com/doc/HACK-Q1-2020--A2FzQJwlu4mWkTIUmB7gSH0RAg-zuTZhovLYbSAFzktgW3SN).\n",
    "\n",
    "**Boolean tag categories:**\n",
    "- \"isBLM\"\n",
    "- \"isBrexit\"\n",
    "- \"isCovid\"\n",
    "- \"isEducation\"\n",
    "- \"isImmigration\"\n",
    "- \"isEconomy\"\n",
    "- \"isProtest\"\n",
    "- \"isRacial\"\n",
    "- \"isLawAndOrder\"\n",
    "\n",
    "**Methods of asserting category membership:**\n",
    "1. Search existing article tags for category keywords\n",
    "2. Search article headline / summary / text for category keywords\n",
    "3. Use [Mango](http://api.mango-en.virt.ch.bbc.co.uk/) or [Starfruit](http://starfruit.virt.ch.bbc.co.uk/) to auto-generate tags, and search these for category keywords\n",
    "4. Use NLI to infer category membership (useful for more vague categories where keyword searches don't suffice)\n",
    "\n",
    "**Note:** you can find available BBC content tags [here](https://www.bbc.co.uk/things/search?q=immigration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category labelling using keyword searches\n",
    "\n",
    "Membership of categories with distinctive associated keywords can be determined by searching for these keywords within the article headline, summary, body, or tags. The dictionary below outlines the categories for which this approach might be possible, and their associated keywords. We may want to exclude certain articles if a keyword appears in conjunction with another term (e.g. and article containing the word \"sand\" might be excluded from the category \"beach\" if it also contains the word \"sandpaper\"). For this reason, each category is associated with a list of \"should_not\" words as well as the \"should\" keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# associate boolean categories with content tags\n",
    "\n",
    "category_tags = {\n",
    "    'isBLM': {'should': ['Black Lives Matter', 'BLM'], \n",
    "              'should_not': []},\n",
    "    'isBrexit': {'should':['Brexit', 'Operation Yellowhammer'], \n",
    "                 'should_not': []},\n",
    "    'isCovid': {'should': ['Covid', 'Coronavirus', 'Self-isolation', 'Lockdown', 'Contact tracing', 'Mers virus', 'Joint Biosecurity Centre (JBC)'],\n",
    "                'should_not': []},\n",
    "    'isEducation': {'should': ['Education'], \n",
    "                    'should_not': []},\n",
    "    'isImmigration': {'should': ['Immigration'], \n",
    "                      'should_not': []},\n",
    "    \"isEconomy\": {'should': ['Economy'], \n",
    "                  'should_not': []},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# load content data\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "CONTENT_ROOT = '/Users/fitzma02/Documents/work/data/all_content'\n",
    "\n",
    "\n",
    "# copied from garden_shed.data_utils.data_io\n",
    "def get_filepaths_in_directory_and_subdirs(root, extension_filter=''):\n",
    "    fpaths = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        for fname in filenames:\n",
    "            if not extension_filter or fname.lower().endswith(extension_filter.lower()):\n",
    "                fpaths.append(os.path.join(dirpath, fname))\n",
    "    return fpaths\n",
    "\n",
    "\n",
    "def load_json_data_from_root(root, extension_filter='', limit=None):\n",
    "    file_paths = get_filepaths_in_directory_and_subdirs(root, extension_filter)\n",
    "    if limit:\n",
    "        file_paths = file_paths[:np.minimum(limit, len(file_paths))]\n",
    "    data = {}\n",
    "    for fpath in file_paths:\n",
    "        with open(fpath) as fin:\n",
    "            data[fpath] = json.load(fin)\n",
    "    return data\n",
    "\n",
    "\n",
    "content = load_json_data_from_root(CONTENT_ROOT, extension_filter='', limit=100)\n",
    "print(len(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'urn:bbc:ares::asset:news/uk-scotland-north-east-orkney-shetland-49125777',\n",
       " 'locators': {'assetUri': '/news/uk-scotland-north-east-orkney-shetland-49125777',\n",
       "  'cpsUrn': 'urn:bbc:content:assetUri:news/uk-scotland-north-east-orkney-shetland-49125777',\n",
       "  'curie': 'http://www.bbc.co.uk/asset/a20a180d-8f69-6e4a-8d49-1a3c29c46955'},\n",
       " 'type': 'STY',\n",
       " 'createdBy': 'news',\n",
       " 'language': 'en-gb',\n",
       " 'lastUpdated': 1564134858330,\n",
       " 'firstPublished': 1564134694,\n",
       " 'lastPublished': 1564134849,\n",
       " 'options': {'isIgorSeoTagsEnabled': False,\n",
       "  'includeComments': False,\n",
       "  'allowRightHandSide': True,\n",
       "  'isFactCheck': False,\n",
       "  'allowDateStamp': True,\n",
       "  'suitableForSyndication': True,\n",
       "  'hasNewsTracker': False,\n",
       "  'allowRelatedStoriesBox': True,\n",
       "  'isKeyContent': False,\n",
       "  'allowHeadline': True,\n",
       "  'allowAdvertising': True,\n",
       "  'hasContentWarning': False,\n",
       "  'isBreakingNews': False,\n",
       "  'allowPrintingSharingLinks': True},\n",
       " 'analyticsLabels': {'cps_asset_type': 'sty',\n",
       "  'counterName': 'news.scotland.north_east_orkney_and_shetland.story.49125777.page',\n",
       "  'cps_asset_id': '49125777'},\n",
       " 'passport': {'category': {'categoryId': 'http://www.bbc.co.uk/ontologies/applicationlogic-news/News',\n",
       "   'categoryName': 'News'}},\n",
       " 'tags': {'about': [{'thingLabel': 'City of Aberdeen',\n",
       "    'thingUri': 'http://www.bbc.co.uk/things/2bd31405-8221-4dd0-91b5-5d31ff4d1bd6#id',\n",
       "    'thingId': '2bd31405-8221-4dd0-91b5-5d31ff4d1bd6',\n",
       "    'thingType': ['Place', 'Thing'],\n",
       "    'thingSameAs': ['http://sws.geonames.org/3333224/']},\n",
       "   {'thingLabel': 'Rothienorman',\n",
       "    'thingUri': 'http://www.bbc.co.uk/things/9575da09-189a-45f0-9f92-a84ed0de053c#id',\n",
       "    'thingId': '9575da09-189a-45f0-9f92-a84ed0de053c',\n",
       "    'thingType': ['Place', 'Thing', 'geoname:GeoTagConcept'],\n",
       "    'thingSameAs': ['http://sws.geonames.org/2639079/'],\n",
       "    'topicName': 'Rothienorman',\n",
       "    'topicId': 'cdd52egwej8t',\n",
       "    'curationList': [{'curationId': '2639079',\n",
       "      'curationType': 'location-stream'}]}]},\n",
       " 'version': 'v1.0.10',\n",
       " 'blockTypes': ['image', 'paragraph', 'list', 'crosshead'],\n",
       " 'includeComments': False}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# pick random test article from content data\n",
    "content_list = list(content.items())\n",
    "test_article = random.choice(content_list)[1]\n",
    "test_article['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract relevant text elements from article\n",
    "\n",
    "\n",
    "def get_tags(article):\n",
    "    \"\"\"\n",
    "    Extract tags from within article json.\n",
    "    \"\"\"\n",
    "    all_tags = article.get('metadata', {}).get('tags', {}).get('about', [])\n",
    "    tag_names = [t.get('thingLabel') for t in all_tags]\n",
    "    return tag_names\n",
    "\n",
    "\n",
    "def get_headline(article):\n",
    "    \"\"\"\n",
    "    Extract headline from within article json.\n",
    "    \"\"\"\n",
    "    return article.get('promo', {}).get('headlines', {}).get('headline', '')\n",
    "\n",
    "\n",
    "def get_summary(article):\n",
    "    \"\"\"\n",
    "    Extract summary from within article json.\n",
    "    \"\"\"\n",
    "    return article.get('promo', {}).get('summary', '')\n",
    "\n",
    "\n",
    "def get_body(article):\n",
    "    \"\"\"\n",
    "    Extract body text from within article json.\n",
    "    \"\"\"\n",
    "    text = ''\n",
    "    for t in article.get('content', {}).get('blocks', []):\n",
    "        text += t.get('text', '')\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_uri(article):\n",
    "    \"\"\"\n",
    "    Extract uri from within article json.\n",
    "    \"\"\"\n",
    "    return article.get('metadata', {}).get('locators', {}).get('assetUri', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tags = get_tags(test_article)\n",
    "test_headline = get_headline(test_article)\n",
    "test_summary = get_summary(test_article)\n",
    "test_uri = get_uri(test_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_text_for_keywords(text, category_tags):\n",
    "    \"\"\"\n",
    "    Function checks if text includes category tags, excluding invalid tags.\n",
    "    Args:\n",
    "        text (list): list of text segments to be searched (these may be tags, headlines, or body text)\n",
    "        category_tags (dict): dictionary containing a list of \"should\" tags, and optional \"should_not\" tags\n",
    "    Returns:\n",
    "        True if text contains at least one valid category tag, and no invalid tags.\n",
    "    \"\"\"\n",
    "    all_text = (' ').join([t.lower() for t in text])\n",
    "    should = [t.lower() for t in category_tags.get('should', [])]\n",
    "    should_not = [t.lower() for t in category_tags.get('should_not', [])]\n",
    "    \n",
    "    for s in should_not:\n",
    "        if all_text.find(s) >= 0:\n",
    "            return False\n",
    "    for s in should:\n",
    "        if all_text.find(s) >= 0:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text = test_tags + [test_headline, test_summary]\n",
    "check_text_for_keywords(all_text, {'should': ['Entertainment', 'Scotland'], 'should_not': ['Brexit']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "starfruit_api = 'http://starfruit.virt.ch.bbc.co.uk'\n",
    "mango_api = 'http://api.mango-en.virt.ch.bbc.co.uk'\n",
    "\n",
    "\n",
    "def get_results_from_autotagger(asset_uri, api=starfruit_api):\n",
    "    \"\"\"\n",
    "    Query starfruit or mango api with article URI to return auto-generated content tags.\n",
    "    \"\"\"\n",
    "    response = requests.get(f'{api}/topics?uri=https://www.bbc.co.uk{asset_uri}')\n",
    "    body = response.content\n",
    "    return json.loads(body.decode(\"utf-8\"))\n",
    "\n",
    "\n",
    "def parse_labels_from_starfruit_response(response):\n",
    "    \"\"\"\n",
    "    Extract list of auto-generated labels from starfruit api response.\n",
    "    \"\"\"\n",
    "    all_labels = response.get('results', [])\n",
    "    return [l.get('label', {}).get('en-gb', '') for l in all_labels]\n",
    "\n",
    "\n",
    "def parse_labels_from_mango_response(response):\n",
    "    \"\"\"\n",
    "    Extract list of auto-generated labels from mango api response.\n",
    "    \"\"\"\n",
    "    all_labels = response.get('results', [])\n",
    "    return [l.get('label', {}) for l in all_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrested\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Arrested',\n",
       " 'Stabbed',\n",
       " 'Bessbrook',\n",
       " 'murder',\n",
       " 'Police',\n",
       " 'Coleraine',\n",
       " 'BBC News',\n",
       " 'Hospital',\n",
       " 'Robbery',\n",
       " 'Gang',\n",
       " 'Anniversary',\n",
       " 'County Armagh',\n",
       " 'PSNI',\n",
       " 'Prosecution',\n",
       " 'Crimestoppers']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = get_results_from_autotagger(test_uri, api=mango_api)\n",
    "parse_labels_from_mango_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_to_end_labelling(article, category_tags, use_tags=True, use_headline=True, use_summary=True, use_body=True, use_starfish=True, use_mango=True):\n",
    "    \"\"\"\n",
    "    Orchestration of boolean labelling for a single article and category.\n",
    "    args:\n",
    "        article (dict): article object to be labelled\n",
    "        category_tags (dict): dictionary containing a list of \"should\" tags, and optional \"should_not\" tags\n",
    "    Returns:\n",
    "        True if text returned from various optional locations (tags, headline, summary, body, auto-taggers) contains at least one valid category tag, \n",
    "        and no invalid tags.\n",
    "    \"\"\"\n",
    "    text = []\n",
    "    if use_tags:\n",
    "        text += get_tags(article)\n",
    "    if use_headline:\n",
    "        text += [get_headline(article)]\n",
    "    if use_summary:\n",
    "        text += [get_summary(article)]\n",
    "    if use_body:\n",
    "        text += [get_body(article)]\n",
    "    if use_starfish or use_mango:\n",
    "        uri = get_uri(article)\n",
    "        if use_starfish:\n",
    "            response = get_results_from_autotagger(uri, api='http://starfruit.virt.ch.bbc.co.uk')\n",
    "            text += parse_labels_from_starfruit_response(response)\n",
    "        if use_mango:\n",
    "            response = get_results_from_autotagger(uri, api='http://api.mango-en.virt.ch.bbc.co.uk')\n",
    "            text += parse_labels_from_mango_response(response)\n",
    "    return check_text_for_keywords(text, category_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_to_end_labelling(test_article, category_tags['isImmigration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category labelling for vague categories\n",
    "\n",
    "For the \"fuzzier\" categories such as \"isRacial\", \"isProtest\", and \"isLawAndOrder\", keyword searches won't suffice for determining the label. Instead, we will attempt to use NLI to determine membership of these classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out RobertaForQuestionAnswering\n",
    "\n",
    "#!pip install transformers\n",
    "#!pip install transformers[tf-cpu]\n",
    "#!pip install transformers[torch]\n",
    "import tensorflow as tf\n",
    "from transformers import RobertaTokenizer, TFRobertaForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ġto Ġdescribe Ġhim . ĠReally . ĠHe 's Ġthe Ġbest Ġperson ĠI Ġever Ġknew . & qu ot ; Tom ĠSinn ott Ġis Ġplanning Ġhis Ġyounger Ġbrother 's Ġfuneral . The Ġlast Ġthing Ġhe Ġexpected Ġwas Ġjust Ġabout Ġevery Ġprofessional Ġteam Ġin ĠEngland Ġsending Ġhim Ġa Ġshirt Ġwith ĠJordan 's Ġname Ġon Ġthe Ġback Ġof Ġit . But , Ġas Ġhe 's Ġtold ĠRadio Ġ1 ĠNews beat , Ġit 's Ġpart Ġof Ġa Ġtribute Ġto Ġthe Ġ25 - year - old Ġfootballer . Jordan Ġ- Ġa Ġnon - league Ġplayer Ġfor ĠMat lock ĠTown Ġ- Ġdied Ġon ĠSaturday Ġafter Ġbeing Ġattacked Ġduring Ġa Ġnight Ġout . ĠThree Ġ21 - year -\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_text_length = 500\n",
    "question = \"Is race discussed?\"\n",
    "text = get_body(test_article)[:max_text_length]\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = TFRobertaForQuestionAnswering.from_pretrained('roberta-base')\n",
    "input_dict = tokenizer.encode_plus(question, text, return_tensors='tf')\n",
    "start_scores, end_scores = model(input_dict)\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_dict[\"input_ids\"].numpy()[0])\n",
    "answer = ' '.join(all_tokens[tf.math.argmax(start_scores, 1)[0] : tf.math.argmax(end_scores, 1)[0]+1])\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that the label is true: 0.66%\n"
     ]
    }
   ],
   "source": [
    "# Try out HuggingFace facebook/bart-large-mnli\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from nlp.text_entailment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that the label is true: 0.66%\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_bart_model_tokenizer(model_name)\n",
    "premise = \"Oriel College's governors vote to take down the statue of the Victorian colonialist Cecil Rhodes.\"\n",
    "hypothesis = 'References the diamond trade'\n",
    "\n",
    "get_premise_hypothesis_entailment(premise, hypothesis, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify text entailment function to return raw probability\n",
    "\n",
    "def get_premise_hypothesis_entailment_probability(premise, hypothesis, tokenizer, model):\n",
    "    input_ids = tokenizer.encode(premise, hypothesis, return_tensors='pt')\n",
    "    logits = model(input_ids)[0]\n",
    "\n",
    "    # we throw away \"neutral\" (dim 1) and take the probability of\n",
    "    # \"entailment\" (2) as the probability of the label being true\n",
    "    entail_contradiction_logits = logits[:,[0,2]]\n",
    "    probs = entail_contradiction_logits.softmax(dim=1)\n",
    "    true_prob = probs[:,1].item()\n",
    "    return true_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer whether article belongs to category using NLI\n",
    "\n",
    "def get_label_from_entailment(article_text, category_tags, tokenizer, model, threshold=0.5, max_len=1024):\n",
    "    '''\n",
    "    Function applies a binary label to an article about whether it discusses the category, defined by a list of category keywords.\n",
    "    Args: \n",
    "        article_text (str):\n",
    "        category_tags (list): \n",
    "        threshold (float):\n",
    "        tokenizer (BartTokenizer):\n",
    "        model (BartForSequenceClassification):\n",
    "    Returns: \n",
    "        Boolean label indicating whether article belongs to category\n",
    "    '''\n",
    "    hypothesis = f'discusses {category_tags[0]}'\n",
    "    for t in category_tags[1:]:\n",
    "        hypothesis += f' or {t}'\n",
    "    probability = get_premise_hypothesis_entailment_probability(article_text[:np.minimum(max_len, len(article_text))], hypothesis, tokenizer, model)\n",
    "    if probability >= threshold:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = get_body(test_article)\n",
    "get_label_from_entailment(test_text, ['race', 'racism', 'BLM'], tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tuning and evaluation\n",
    "\n",
    "#!pip install sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def end_to_end_labelling_entailment(content_item, category_tags, tokenizer, model, threshold):\n",
    "    \"\"\"\n",
    "    Wrap together get_label_from_entailment with get_body so that it can take raw articles that haven't been pre-processed.\n",
    "    \"\"\"\n",
    "    article_text = get_body(content_item)\n",
    "    return get_label_from_entailment(article_text, category_tags, tokenizer, model, threshold=threshold)\n",
    "\n",
    "\n",
    "def calculate_labels(content, category_tags, prediction_function, **kwargs):\n",
    "    \"\"\"\n",
    "    Iterate through content items and predict label for each of the articles.\n",
    "    \"\"\"\n",
    "    y = []\n",
    "    for k, v in content.items(): \n",
    "        y.append(prediction_function(v, category_tags, **kwargs))\n",
    "    return y\n",
    "\n",
    "\n",
    "def iterate_over_threshold_vals(content, category_tags, tokenizer, model, threshold_vec):\n",
    "    \"\"\"\n",
    "    Iterate over entailment threshold values and return predictions and metrics.\n",
    "    \"\"\"\n",
    "    metrics = pd.DataFrame()\n",
    "    all_y_pred = {}\n",
    "    y = calculate_labels(content, category_tags, end_to_end_labelling)\n",
    "    for t in threshold_vec:\n",
    "        y_pred = calculate_labels(content, category_tags['should'], end_to_end_labelling_entailment, tokenizer=tokenizer, model=model, threshold=t)\n",
    "        all_y_pred[t] = y_pred\n",
    "        tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "        results = {'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp}\n",
    "        results['TPR'] = results['TP'] / (results['TP'] + results['FN'])\n",
    "        results['FPR'] = results['FP'] / (results['TN'] + results['FP'])\n",
    "        metrics = metrics.append(results, ignore_index=True)\n",
    "    return y, all_y_pred, metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "threshold_vals = [0.3, 0.6, 0.9]\n",
    "y, all_y_pred, metrics = iterate_over_threshold_vals(content, category_tags['isCovid'], tokenizer, model, threshold_vals)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(metrics.FPR, metrics.TPR)\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_vals = [0.3, 0.6, 0.9]\n",
    "y, all_y_pred, metrics = iterate_over_threshold_vals(content, category_tags['isCovid'], tokenizer, model, threshold_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(metrics.FPR, metrics.TPR)\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}