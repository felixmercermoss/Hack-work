{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign boolean category labels to news articles\n",
    "\n",
    "### Datascapes Hack, June 2020\n",
    "Hack documentation can be found [here](https://paper.dropbox.com/doc/HACK-Q1-2020--A2FzQJwlu4mWkTIUmB7gSH0RAg-zuTZhovLYbSAFzktgW3SN).\n",
    "\n",
    "**Boolean tag categories:**\n",
    "- \"isBLM\"\n",
    "- \"isBrexit\"\n",
    "- \"isCovid\"\n",
    "- \"isEducation\"\n",
    "- \"isImmigration\"\n",
    "- \"isEconomy\"\n",
    "- \"isProtest\"\n",
    "- \"isRacial\"\n",
    "- \"isLawAndOrder\"\n",
    "\n",
    "**Methods of asserting category membership:**\n",
    "1. Search existing article tags for category keywords\n",
    "2. Search article headline / summary / text for category keywords\n",
    "3. Use [Mango](http://api.mango-en.virt.ch.bbc.co.uk/) or [Starfruit](http://starfruit.virt.ch.bbc.co.uk/) to auto-generate tags, and search these for category keywords\n",
    "\n",
    "**Note:** you can find available BBC content tags [here](https://www.bbc.co.uk/things/search?q=immigration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57819\n"
     ]
    }
   ],
   "source": [
    "# load content data\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "#CONTENT_ROOT = '/Users/fitzma02/Documents/work/data/all_content'\n",
    "\n",
    "\n",
    "\n",
    "# copied from garden_shed.data_utils.data_io\n",
    "def get_filepaths_in_directory_and_subdirs(root, extension_filter=''):\n",
    "    fpaths = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        for fname in filenames:\n",
    "            if not extension_filter or fname.lower().endswith(extension_filter.lower()):\n",
    "                fpaths.append(os.path.join(dirpath, fname))\n",
    "    return fpaths\n",
    "\n",
    "\n",
    "def load_json_data_from_root(root, extension_filter=''):\n",
    "    file_paths = get_filepaths_in_directory_and_subdirs(root, extension_filter)\n",
    "    data = {}\n",
    "    for fpath in file_paths:\n",
    "        with open(fpath) as fin:\n",
    "            data[fpath] = json.load(fin)\n",
    "    return data\n",
    "\n",
    "\n",
    "content = load_json_data_from_root(CONTENT_ROOT, extension_filter='')\n",
    "print(len(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'about': [{'thingLabel': 'Matlock',\n",
       "   'thingUri': 'http://www.bbc.co.uk/things/b8415938-21af-4977-bcec-a6e460a751bc#id',\n",
       "   'thingId': 'b8415938-21af-4977-bcec-a6e460a751bc',\n",
       "   'thingType': ['Thing', 'Place', 'geoname:GeoTagConcept'],\n",
       "   'thingSameAs': ['http://sws.geonames.org/2642910/']},\n",
       "  {'thingLabel': 'Jordan Sinnott death',\n",
       "   'thingUri': 'http://www.bbc.co.uk/things/cb83cb53-4bd3-4b8e-ae9d-e1089cf505fa#id',\n",
       "   'thingId': 'cb83cb53-4bd3-4b8e-ae9d-e1089cf505fa',\n",
       "   'thingType': ['Thing', 'Event'],\n",
       "   'thingSameAs': ['http://dbpedia.org/resource/Jordan_Sinnott',\n",
       "    'http://www.wikidata.org/entity/Q6277016']},\n",
       "  {'thingLabel': 'Derbyshire',\n",
       "   'thingUri': 'http://www.bbc.co.uk/things/f1cf068a-f25a-4c04-ab12-99e16f6c61b8#id',\n",
       "   'thingId': 'f1cf068a-f25a-4c04-ab12-99e16f6c61b8',\n",
       "   'thingType': ['core:CeremonialCounty', 'Thing', 'Place'],\n",
       "   'thingSameAs': ['http://dbpedia.org/resource/Derbyshire']}]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# pick random test article from content data\n",
    "content_list = list(content.items())\n",
    "test_article = random.choice(content_list)[1]\n",
    "test_article['metadata']['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# associate boolean categories with content tags\n",
    "\n",
    "category_tags = {\n",
    "    'isBLM': {'should': ['Black Lives Matter', 'BLM'], \n",
    "              'should_not': []},\n",
    "    'isBrexit': {'should':['Brexit', 'Operation Yellowhammer'], \n",
    "                 'should_not': []},\n",
    "    'isCovid': {'should': ['Covid', 'Coronavirus', 'Self-isolation', 'Lockdown', 'Contact tracing', 'Mers virus', 'Joint Biosecurity Centre (JBC)'],\n",
    "                'should_not': []},\n",
    "    'isEducation': {'should': ['Education'], \n",
    "                    'should_not': []},\n",
    "    'isImmigration': {'should': ['Immigration'], \n",
    "                      'should_not': []},\n",
    "    \"isEconomy\": {'should': ['Economy'], \n",
    "                  'should_not': []},\n",
    "    \"isProtest\": {'should': [], \n",
    "                  'should_not': []},\n",
    "    \"isRacial\": {'should': [], \n",
    "                 'should_not': []},\n",
    "    \"isLawAndOrder\": {'should': [], \n",
    "                      'should_not': []},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract relevant text elements from article\n",
    "\n",
    "\n",
    "def get_tags(article):\n",
    "    \"\"\"\n",
    "    Extract tags from within article json.\n",
    "    \"\"\"\n",
    "    all_tags = article.get('metadata', {}).get('tags', {}).get('about', [])\n",
    "    tag_names = [t.get('thingLabel') for t in all_tags]\n",
    "    return tag_names\n",
    "\n",
    "\n",
    "def get_headline(article):\n",
    "    \"\"\"\n",
    "    Extract headline from within article json.\n",
    "    \"\"\"\n",
    "    return article.get('promo', {}).get('headlines', {}).get('headline', '')\n",
    "\n",
    "\n",
    "def get_summary(article):\n",
    "    \"\"\"\n",
    "    Extract summary from within article json.\n",
    "    \"\"\"\n",
    "    return article.get('promo', {}).get('summary', '')\n",
    "\n",
    "\n",
    "def get_body(article):\n",
    "    \"\"\"\n",
    "    Extract body text from within article json.\n",
    "    \"\"\"\n",
    "    text = ''\n",
    "    for t in article.get('content', {}).get('blocks', []):\n",
    "        text += t.get('text', '')\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_uri(article):\n",
    "    \"\"\"\n",
    "    Extract uri from within article json.\n",
    "    \"\"\"\n",
    "    return article.get('metadata', {}).get('locators', {}).get('assetUri', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tags = get_tags(test_article)\n",
    "test_headline = get_headline(test_article)\n",
    "test_summary = get_summary(test_article)\n",
    "test_uri = get_uri(test_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_text_for_keywords(text, category_tags):\n",
    "    \"\"\"\n",
    "    Function checks if text includes category tags, excluding invalid tags.\n",
    "    Args:\n",
    "        text (list): list of text segments to be searched (these may be tags, headlines, or body text)\n",
    "        category_tags (dict): dictionary containing a list of \"should\" tags, and optional \"should_not\" tags\n",
    "    Returns:\n",
    "        True if text contains at least one valid category tag, and no invalid tags.\n",
    "    \"\"\"\n",
    "    all_text = (' ').join([t.lower() for t in text])\n",
    "    should = [t.lower() for t in category_tags.get('should', [])]\n",
    "    should_not = [t.lower() for t in category_tags.get('should_not', [])]\n",
    "    \n",
    "    for s in should_not:\n",
    "        if all_text.find(s) >= 0:\n",
    "            return False\n",
    "    for s in should:\n",
    "        if all_text.find(s) >= 0:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text = test_tags + [test_headline, test_summary]\n",
    "check_text_for_keywords(all_text, {'should': ['Entertainment', 'Scotland'], 'should_not': ['Brexit']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "starfruit_api = 'http://starfruit.virt.ch.bbc.co.uk'\n",
    "mango_api = 'http://api.mango-en.virt.ch.bbc.co.uk'\n",
    "\n",
    "\n",
    "def get_results_from_autotagger(asset_uri, api=starfruit_api):\n",
    "    \"\"\"\n",
    "    Query starfruit or mango api with article URI to return auto-generated content tags.\n",
    "    \"\"\"\n",
    "    response = requests.get(f'{api}/topics?uri=https://www.bbc.co.uk{asset_uri}')\n",
    "    body = response.content\n",
    "    return json.loads(body.decode(\"utf-8\"))\n",
    "\n",
    "\n",
    "def parse_labels_from_starfruit_response(response):\n",
    "    \"\"\"\n",
    "    Extract list of auto-generated labels from starfruit api response.\n",
    "    \"\"\"\n",
    "    all_labels = response.get('results', [])\n",
    "    return [l.get('label', {}).get('en-gb', '') for l in all_labels]\n",
    "\n",
    "\n",
    "def parse_labels_from_mango_response(response):\n",
    "    \"\"\"\n",
    "    Extract list of auto-generated labels from mango api response.\n",
    "    \"\"\"\n",
    "    all_labels = response.get('results', [])\n",
    "    return [l.get('label', {}) for l in all_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrested\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Arrested',\n",
       " 'Stabbed',\n",
       " 'Bessbrook',\n",
       " 'murder',\n",
       " 'Police',\n",
       " 'Coleraine',\n",
       " 'BBC News',\n",
       " 'Hospital',\n",
       " 'Robbery',\n",
       " 'Gang',\n",
       " 'Anniversary',\n",
       " 'County Armagh',\n",
       " 'PSNI',\n",
       " 'Prosecution',\n",
       " 'Crimestoppers']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = get_results_from_autotagger(test_uri, api=mango_api)\n",
    "parse_labels_from_mango_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_to_end_labelling(article, category_tags, use_tags=True, use_headline=True, use_summary=True, use_body=True, use_starfish=True, use_mango=True):\n",
    "    \"\"\"\n",
    "    Orchestration of boolean labelling for a single article and category.\n",
    "    args:\n",
    "        article (dict): article object to be labelled\n",
    "        category_tags (dict): dictionary containing a list of \"should\" tags, and optional \"should_not\" tags\n",
    "    Returns:\n",
    "        True if text returned from various optional locations (tags, headline, summary, body, auto-taggers) contains at least one valid category tag, \n",
    "        and no invalid tags.\n",
    "    \"\"\"\n",
    "    text = []\n",
    "    if use_tags:\n",
    "        text += get_tags(article)\n",
    "    if use_headline:\n",
    "        text += [get_headline(article)]\n",
    "    if use_summary:\n",
    "        text += [get_summary(article)]\n",
    "    if use_body:\n",
    "        text += [get_body(article)]\n",
    "    if use_starfish or use_mango:\n",
    "        uri = get_uri(article)\n",
    "        if use_starfish:\n",
    "            response = get_results_from_autotagger(uri, api='http://starfruit.virt.ch.bbc.co.uk')\n",
    "            text += parse_labels_from_starfruit_response(response)\n",
    "        if use_mango:\n",
    "            response = get_results_from_autotagger(uri, api='http://api.mango-en.virt.ch.bbc.co.uk')\n",
    "            text += parse_labels_from_mango_response(response)\n",
    "    return check_text_for_keywords(text, category_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_to_end_labelling(test_article, category_tags['isImmigration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-2.11.0-py3-none-any.whl (674 kB)\n",
      "\u001b[K     |████████████████████████████████| 674 kB 710 kB/s eta 0:00:01     |███████████████████             | 399 kB 710 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers==0.7.0\n",
      "  Downloading tokenizers-0.7.0-cp37-cp37m-macosx_10_10_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 397 kB/s eta 0:00:01     |████▊                           | 174 kB 397 kB/s eta 0:00:03     |██████████▏                     | 378 kB 397 kB/s eta 0:00:03\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.91-cp37-cp37m-macosx_10_6_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 3.4 MB/s eta 0:00:01     |██████████                      | 348 kB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: numpy in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from transformers) (4.42.1)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 13.1 MB/s eta 0:00:01     |███████████████▏                | 419 kB 13.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from transformers) (20.1)\n",
      "Requirement already satisfied: filelock in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2020.6.8.tar.gz (690 kB)\n",
      "\u001b[K     |████████████████████████████████| 690 kB 486 kB/s eta 0:00:01     |█████████████████               | 368 kB 486 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: six in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied: click in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from packaging->transformers) (2.4.6)\n",
      "Building wheels for collected packages: sacremoses, regex\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=de248269a15089a8fa3d6b5aa844c69788a5be17962bccbb6c64bf022446e0ac\n",
      "  Stored in directory: /Users/gilmoc04/Library/Caches/pip/wheels/69/09/d1/bf058f7d6fa0ecba2ce7c66be3b8d012beb4bf61a6e0c101c0\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for regex: filename=regex-2020.6.8-cp37-cp37m-macosx_10_9_x86_64.whl size=282604 sha256=19793ec39db1d15d9bb05992ff0a4b27b256084eadfb3397de80bc934e60ba2b\n",
      "  Stored in directory: /Users/gilmoc04/Library/Caches/pip/wheels/46/f1/0b/a372e98f7103934a3573301c71b475143baf8ba6f6dffc876c\n",
      "Successfully built sacremoses regex\n",
      "Installing collected packages: tokenizers, sentencepiece, regex, sacremoses, transformers\n",
      "Successfully installed regex-2020.6.8 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n",
      "Requirement already satisfied: transformers[torch] in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (2.11.0)\n",
      "Requirement already satisfied: requests in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from transformers[torch]) (2.22.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from transformers[torch]) (0.1.91)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from transformers[torch]) (4.42.1)\n",
      "Requirement already satisfied: sacremoses in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from transformers[torch]) (0.0.43)\n",
      "Requirement already satisfied: filelock in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from transformers[torch]) (3.0.12)\n",
      "Requirement already satisfied: numpy in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from transformers[torch]) (1.18.1)\n",
      "Requirement already satisfied: packaging in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from transformers[torch]) (20.1)\n",
      "Requirement already satisfied: tokenizers==0.7.0 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from transformers[torch]) (0.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from transformers[torch]) (2020.6.8)\n",
      "Collecting torch; extra == \"torch\"\n",
      "  Downloading torch-1.5.1-cp37-none-macosx_10_9_x86_64.whl (80.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 80.5 MB 1.2 MB/s eta 0:00:011   |                                | 174 kB 460 kB/s eta 0:02:55     |▎                               | 819 kB 460 kB/s eta 0:02:54     |▉                               | 2.1 MB 629 kB/s eta 0:02:05     |█▍                              | 3.6 MB 364 kB/s eta 0:03:31     |██▌                             | 6.2 MB 1.7 MB/s eta 0:00:45     |███▏                            | 7.9 MB 1.5 MB/s eta 0:00:50     |███▎                            | 8.1 MB 1.5 MB/s eta 0:00:49     |█████▎                          | 13.4 MB 368 kB/s eta 0:03:03     |█████▊                          | 14.4 MB 996 kB/s eta 0:01:07     |██████▌                         | 16.3 MB 3.1 MB/s eta 0:00:21     |███████▌                        | 18.9 MB 1.3 MB/s eta 0:00:47     |████████▊                       | 21.8 MB 5.3 MB/s eta 0:00:11     |█████████                       | 22.4 MB 5.3 MB/s eta 0:00:11     |█████████▋                      | 24.1 MB 1.7 MB/s eta 0:00:34     |█████████▉                      | 24.6 MB 1.0 MB/s eta 0:00:56     |██████████▉                     | 27.3 MB 51 kB/s eta 0:17:06     |█████████████▎                  | 33.5 MB 1.4 MB/s eta 0:00:34     |██████████████▎                 | 36.0 MB 2.1 MB/s eta 0:00:22     |███████████████▌                | 39.1 MB 748 kB/s eta 0:00:56     |████████████████▊               | 42.0 MB 971 kB/s eta 0:00:40     |█████████████████▊              | 44.7 MB 1.3 MB/s eta 0:00:29     |███████████████████▉            | 49.9 MB 862 kB/s eta 0:00:36     |████████████████████▉           | 52.4 MB 193 kB/s eta 0:02:26     |█████████████████████▏          | 53.2 MB 442 kB/s eta 0:01:02     |████████████████████████████▋   | 72.1 MB 1.8 MB/s eta 0:00:05     |█████████████████████████████▊  | 74.8 MB 348 kB/s eta 0:00:17     |██████████████████████████████▌ | 76.8 MB 2.9 MB/s eta 0:00:02     |██████████████████████████████▊ | 77.4 MB 2.9 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from requests->transformers[torch]) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from requests->transformers[torch]) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from requests->transformers[torch]) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from requests->transformers[torch]) (3.0.4)\n",
      "Requirement already satisfied: click in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers[torch]) (7.0)\n",
      "Requirement already satisfied: six in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers[torch]) (1.14.0)\n",
      "Requirement already satisfied: joblib in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers[torch]) (0.14.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from packaging->transformers[torch]) (2.4.6)\n",
      "Requirement already satisfied: future in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from torch; extra == \"torch\"->transformers[torch]) (0.18.2)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install transformers[torch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[tf-cpu] in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (2.11.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from transformers[tf-cpu]) (4.42.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from transformers[tf-cpu]) (2020.6.8)\n",
      "Requirement already satisfied: sacremoses in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from transformers[tf-cpu]) (0.0.43)\n",
      "Requirement already satisfied: tokenizers==0.7.0 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from transformers[tf-cpu]) (0.7.0)\n",
      "Requirement already satisfied: requests in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from transformers[tf-cpu]) (2.22.0)\n",
      "Requirement already satisfied: packaging in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from transformers[tf-cpu]) (20.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from transformers[tf-cpu]) (0.1.91)\n",
      "Requirement already satisfied: numpy in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from transformers[tf-cpu]) (1.18.1)\n",
      "Requirement already satisfied: filelock in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from transformers[tf-cpu]) (3.0.12)\n",
      "Collecting onnxconverter-common; extra == \"tf-cpu\"\n",
      "  Downloading onnxconverter_common-1.7.0-py2.py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras2onnx; extra == \"tf-cpu\"\n",
      "  Downloading keras2onnx-1.7.0-py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 604 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-cpu; extra == \"tf-cpu\"\n",
      "  Downloading tensorflow_cpu-2.2.0-cp37-cp37m-macosx_10_11_x86_64.whl (175.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 175.3 MB 24 kB/s  eta 0:00:012 |                                | 30 kB 391 kB/s eta 0:07:28     |▍                               | 2.1 MB 631 kB/s eta 0:04:35     |▌                               | 2.7 MB 631 kB/s eta 0:04:34     |█▌                              | 8.2 MB 395 kB/s eta 0:07:02     |██▎                             | 12.3 MB 2.8 MB/s eta 0:00:59     |██▋                             | 14.3 MB 359 kB/s eta 0:07:29     |███▌                            | 19.2 MB 543 kB/s eta 0:04:48     |███▋                            | 19.9 MB 543 kB/s eta 0:04:47     |███▉                            | 20.9 MB 640 kB/s eta 0:04:02     |████                            | 22.3 MB 3.0 MB/s eta 0:00:52     |████▌                           | 24.7 MB 205 kB/s eta 0:12:14     |█████                           | 27.7 MB 113 kB/s eta 0:21:44     |█████▋                          | 30.5 MB 181 kB/s eta 0:13:17     |██████                          | 33.3 MB 202 kB/s eta 0:11:42     |██████▌                         | 35.4 MB 378 kB/s eta 0:06:10     |███████                         | 38.6 MB 3.3 MB/s eta 0:00:42     |████████                        | 43.3 MB 458 kB/s eta 0:04:48     |████████▏                       | 44.8 MB 617 kB/s eta 0:03:32     |█████████▎                      | 50.9 MB 422 kB/s eta 0:04:55     |█████████▋                      | 52.4 MB 1.3 MB/s eta 0:01:33     |██████████▋                     | 58.2 MB 1.8 MB/s eta 0:01:06     |██████████▊                     | 58.4 MB 1.8 MB/s eta 0:01:06  | 61.2 MB 876 kB/s eta 0:02:11     |████████████▊                   | 69.8 MB 823 kB/s eta 0:02:09     |█████████████                   | 71.4 MB 1.0 MB/s eta 0:01:42     |███████████████                 | 81.7 MB 1.0 MB/s eta 0:01:31     |███████████████                 | 82.6 MB 666 kB/s eta 0:02:20     |████████████████                | 87.7 MB 1.2 MB/s eta 0:01:14     |████████████████                | 88.1 MB 1.2 MB/s eta 0:01:13     |████████████████▏               | 88.5 MB 1.2 MB/s eta 0:01:13     |████████████████▊               | 91.5 MB 4.3 MB/s eta 0:00:20     |█████████████████               | 93.5 MB 4.3 MB/s eta 0:00:19     |█████████████████▎              | 94.7 MB 1.5 MB/s eta 0:00:56     |███████████████████▉            | 108.7 MB 3.5 MB/s eta 0:00:19     |████████████████████            | 109.4 MB 733 kB/s eta 0:01:30     |████████████████████▍           | 111.5 MB 991 kB/s eta 0:01:05     |████████████████████▌           | 112.4 MB 991 kB/s eta 0:01:04     |████████████████████▊           | 113.3 MB 3.3 MB/s eta 0:00:19     |████████████████████▉           | 113.9 MB 3.3 MB/s eta 0:00:19     |█████████████████████▏          | 115.7 MB 2.9 MB/s eta 0:00:21     |██████████████████████▎         | 122.2 MB 1.9 MB/s eta 0:00:28     |████████████████████████▉       | 136.1 MB 767 kB/s eta 0:00:52     |███████████████████████████     | 148.1 MB 1.7 MB/s eta 0:00:17     |███████████████████████████▌    | 150.8 MB 684 kB/s eta 0:00:36     |███████████████████████████▊    | 151.6 MB 759 kB/s eta 0:00:32     |████████████████████████████    | 153.0 MB 3.6 MB/s eta 0:00:07     |████████████████████████████▎   | 155.0 MB 11.6 MB/s eta 0:00:02     |████████████████████████████▋   | 156.5 MB 11.6 MB/s eta 0:00:02     |█████████████████████████████▌  | 161.8 MB 634 kB/s eta 0:00:22     |█████████████████████████████▉  | 163.4 MB 432 kB/s eta 0:00:28     |██████████████████████████████▏ | 165.5 MB 1.0 MB/s eta 0:00:10     |███████████████████████████████ | 170.0 MB 985 kB/s eta 0:00:06\n",
      "\u001b[?25hRequirement already satisfied: joblib in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers[tf-cpu]) (0.14.1)\n",
      "Requirement already satisfied: click in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers[tf-cpu]) (7.0)\n",
      "Requirement already satisfied: six in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers[tf-cpu]) (1.14.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from requests->transformers[tf-cpu]) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from requests->transformers[tf-cpu]) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from requests->transformers[tf-cpu]) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from requests->transformers[tf-cpu]) (1.25.8)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from packaging->transformers[tf-cpu]) (2.4.6)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-3.12.2-cp37-cp37m-macosx_10_9_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting onnx\n",
      "  Downloading onnx-1.7.0-cp37-cp37m-macosx_10_9_x86_64.whl (7.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.5 MB 166 kB/s eta 0:00:01     |███████▋                        | 1.8 MB 622 kB/s eta 0:00:10\n",
      "\u001b[?25hCollecting fire\n",
      "  Downloading fire-0.3.1.tar.gz (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 228 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 591 kB/s eta 0:00:011     |████████████████████████████▌   | 51 kB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from tensorflow-cpu; extra == \"tf-cpu\"->transformers[tf-cpu]) (1.11.2)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from tensorflow-cpu; extra == \"tf-cpu\"->transformers[tf-cpu]) (1.4.1)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.2.1-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "\u001b[K     |████████████████████████████████| 454 kB 430 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 166 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from tensorflow-cpu; extra == \"tf-cpu\"->transformers[tf-cpu]) (2.10.0)\n",
      "Collecting keras-preprocessing>=1.1.0\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 36 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<2.3.0,>=2.2.0\n",
      "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 172 kB/s eta 0:00:01     |███████████████████████████████▋| 2.9 MB 172 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from tensorflow-cpu; extra == \"tf-cpu\"->transformers[tf-cpu]) (0.34.2)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.29.0-cp37-cp37m-macosx_10_9_x86_64.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 339 kB/s eta 0:00:01     |███▌                            | 307 kB 3.9 MB/s eta 0:00:01     |█████▉                          | 501 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from protobuf->onnxconverter-common; extra == \"tf-cpu\"->transformers[tf-cpu]) (46.0.0.post20200309)\n",
      "Collecting typing-extensions>=3.6.2.1\n",
      "  Downloading typing_extensions-3.7.4.2-py3-none-any.whl (22 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 928 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-cpu; extra == \"tf-cpu\"->transformers[tf-cpu]) (1.0.0)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.18.0-py2.py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 588 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl (777 kB)\n",
      "\u001b[K     |████████████████████████████████| 777 kB 121 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-cpu; extra == \"tf-cpu\"->transformers[tf-cpu]) (1.5.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4; python_version >= \"3\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 499 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/gilmoc04/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-cpu; extra == \"tf-cpu\"->transformers[tf-cpu]) (2.2.0)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 2.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: fire, termcolor, absl-py\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=1b75d831b7a1c2a6ff85dbf794b6100b4943fb21344cb79f8abce8e5577d4c08\n",
      "  Stored in directory: /Users/gilmoc04/Library/Caches/pip/wheels/95/38/e1/8b62337a8ecf5728bdc1017e828f253f7a9cf25db999861bec\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=6eada86b09cb8c4222cb2230c8f27734b55662e966908b989a07efa2475debb1\n",
      "  Stored in directory: /Users/gilmoc04/Library/Caches/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=39088141154ad03ac4d012fd6f12adffbc36494c9d48fcf9cf6ff440f7f74348\n",
      "  Stored in directory: /Users/gilmoc04/Library/Caches/pip/wheels/cc/af/1a/498a24d0730ef484019e007bb9e8cef3ac00311a672c049a3e\n",
      "Successfully built fire termcolor absl-py\n",
      "Installing collected packages: protobuf, typing-extensions, onnx, onnxconverter-common, termcolor, fire, keras2onnx, google-pasta, gast, opt-einsum, tensorflow-estimator, astunparse, absl-py, keras-preprocessing, grpcio, markdown, oauthlib, requests-oauthlib, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, google-auth-oauthlib, tensorboard-plugin-wit, tensorboard, tensorflow-cpu\n",
      "Successfully installed absl-py-0.9.0 astunparse-1.6.3 cachetools-4.1.0 fire-0.3.1 gast-0.3.3 google-auth-1.18.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.29.0 keras-preprocessing-1.1.2 keras2onnx-1.7.0 markdown-3.2.2 oauthlib-3.1.0 onnx-1.7.0 onnxconverter-common-1.7.0 opt-einsum-3.2.1 protobuf-3.12.2 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 tensorboard-2.2.2 tensorboard-plugin-wit-1.6.0.post3 tensorflow-cpu-2.2.0 tensorflow-estimator-2.2.0 termcolor-1.1.0 typing-extensions-3.7.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[tf-cpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import RobertaTokenizer, TFRobertaForQuestionAnswering\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = TFRobertaForQuestionAnswering.from_pretrained('roberta-base')\n",
    "question, text = \"when was the search terminated?\", \"A search for a person missing in the sea off Hampshire has been called off. A woman was rescued and a search was under way for a person said to be in difficulty at Langstone Harbour, near Hayling Island, at about 13:25 GMT. The woman was taken to hospital while four lifeboat crews and a coastguard helicopter continued investigating. The coastguard said nothing had been found and the search was 'terminated pending further information' at about 19:00 GMT. Alan Barnett, from Hayling Island Lifeboat Station, described it as 'quite a big search operation'\"\n",
    "input_dict = tokenizer.encode_plus(question, text, return_tensors='tf')\n",
    "start_scores, end_scores = model(input_dict)\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_dict[\"input_ids\"].numpy()[0])\n",
    "answer = ' '.join(all_tokens[tf.math.argmax(start_scores, 1)[0] : tf.math.argmax(end_scores, 1)[0]+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ġcalled Ġoff . ĠA Ġwoman Ġwas Ġrescued Ġand Ġa Ġsearch Ġwas Ġunder Ġway Ġfor Ġa Ġperson Ġsaid Ġto Ġbe Ġin Ġdifficulty Ġat ĠLang stone ĠHarbour , Ġnear ĠHay ling ĠIsland , Ġat Ġabout Ġ13 : 25 ĠGMT . ĠThe Ġwoman Ġwas Ġtaken Ġto Ġhospital Ġwhile Ġfour Ġlife boat Ġcrews Ġand Ġa Ġcoast guard Ġhelicopter Ġcontinued Ġinvestigating . ĠThe Ġcoast guard Ġsaid Ġnothing Ġhad Ġbeen Ġfound Ġand Ġthe Ġsearch Ġwas Ġ' termin ated Ġpending Ġfurther Ġinformation ' Ġat Ġabout Ġ19 : 00 ĠGMT . ĠAlan ĠBarnett , Ġfrom ĠHay ling ĠIsland ĠLife boat ĠStation , Ġdescribed Ġit Ġas Ġ' quite Ġa\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155a62536ad8412d837782cfb7777fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a0f71cf54d747e0998f99d102579bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191bf29fe15e449cb66c8942d6b96e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=629.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a556475b988f4114bd5ede258bfe5b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60bfab111a4c4ac48fc4dd6bc0cadf27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267844284.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[{'label': 'NEGATIVE', 'score': 0.9991129040718079}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline; print(pipeline('sentiment-analysis')('I hate you'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'ĠWho',\n",
       " 'Ġwas',\n",
       " 'ĠJim',\n",
       " 'ĠH',\n",
       " 'enson',\n",
       " '?',\n",
       " '</s>',\n",
       " '</s>',\n",
       " 'ĠJim',\n",
       " 'ĠH',\n",
       " 'enson',\n",
       " 'Ġwas',\n",
       " 'Ġa',\n",
       " 'Ġnice',\n",
       " 'Ġpuppet',\n",
       " '</s>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 17), dtype=float32, numpy=\n",
       "array([[ 0.09933002, -0.12340708, -0.13821322,  0.01947924, -0.03826241,\n",
       "         0.05032244, -0.02616964,  0.11487867,  0.13624862, -0.01753932,\n",
       "        -0.04074576,  0.01615429, -0.15535429, -0.06976391, -0.02800934,\n",
       "        -0.00207993,  0.11487873]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 17), dtype=float32, numpy=\n",
       "array([[-0.06370163,  0.1411129 ,  0.16514681, -0.07518349,  0.17826185,\n",
       "        -0.03513342, -0.0654796 , -0.07701048, -0.0368855 , -0.0055783 ,\n",
       "         0.12038381, -0.05984056,  0.13992089,  0.03762101, -0.01167762,\n",
       "        -0.07321434, -0.07701055]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../src/nlp'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_entailment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that the label is true: 61.45%\n"
     ]
    }
   ],
   "source": [
    "#premise = \"Oriel College's governors vote to take down the statue of the Victorian colonialist Cecil Rhodes.\"\n",
    "#hypothesis = 'References the diamond trade'\n",
    "\n",
    "#text_entailment.get_premise_hypothesis_entailment(premise, hypothesis, tokenizer, model)\n",
    "\n",
    "text_entailment.get_premise_hypothesis_entailment(\"A search for a person missing in the sea off Hampshire has been called off. A woman was rescued and a search was under way for a person said to be in difficulty at Langstone Harbour, near Hayling Island, at about 13:25 GMT. The woman was taken to hospital while four lifeboat crews and a coastguard helicopter continued investigating. The coastguard said nothing had been found and the search was 'terminated pending further information' at about 19:00 GMT. Alan Barnett, from Hayling Island Lifeboat Station, described it as 'quite a big search operation'\", \"is about the sea\", tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_premise_hypothesis_entailment_probability(premise, hypothesis, tokenizer, model):\n",
    "    input_ids = tokenizer.encode(premise, hypothesis, return_tensors='pt')\n",
    "    logits = model(input_ids)[0]\n",
    "\n",
    "    # we throw away \"neutral\" (dim 1) and take the probability of\n",
    "    # \"entailment\" (2) as the probability of the label being true\n",
    "    entail_contradiction_logits = logits[:,[0,2]]\n",
    "    probs = entail_contradiction_logits.softmax(dim=1)\n",
    "    true_prob = probs[:,1].item()\n",
    "    return true_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_from_entailment(article_text, category, tokenizer, model, threshold=0.5):\n",
    "    '''\n",
    "    Function applies a binary label to an article about whether it discusses the category\n",
    "    Args: article_text(string), category(string), threshold(float), tokenizer(BartTokenizer), model(BartForSequenceClassification)\n",
    "    Returns: boolean label\n",
    "    '''\n",
    "    hypothesis = f\"discusses {category}\"\n",
    "    probability = get_premise_hypothesis_entailment_probability(article_text, hypothesis, tokenizer, model)\n",
    "    if probability >= threshold:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_article_text = \"A search for a person missing in the sea off Hampshire has been called off. A woman was rescued and a search was under way for a person said to be in difficulty at Langstone Harbour, near Hayling Island, at about 13:25 GMT. The woman was taken to hospital while four lifeboat crews and a coastguard helicopter continued investigating. The coastguard said nothing had been found and the search was 'terminated pending further information' at about 19:00 GMT. Alan Barnett, from Hayling Island Lifeboat Station, described it as 'quite a big search operation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_label_from_entailment(test_article_text, \"penguins\", tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
